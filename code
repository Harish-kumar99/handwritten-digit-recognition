import tensorflow as tf
mnist=tf.keras.datasets.mnist
(x_tr,y_tr),(x_ts,y_ts)=mnist.load_data()
x_tr.shape




import matplotlib.pyplot as plt
plt.imshow(x_tr[0])
plt.show()
#############################
plt.imshow(x_tr[90],cmap=plt.cm.binary)
print(x_tr[0])



#normalizing
x_tr=tf.keras.utils.normalize(x_tr,axis=1)

x_ts=tf.keras.utils.normalize(x_ts,axis=1)
plt.imshow(x_tr[0],cmap=plt.cm.binary)

#values are now bw 1 and 0
print(x_tr[0])


print(y_tr[0])


#makin one more dimenssion to apply convol
import numpy as np
imgsize=28
x_tr=np.array(x_tr).reshape(-1,imgsize,imgsize,1)
x_ts=np.array(x_ts).reshape(-1,imgsize,imgsize,1)
print(x_tr.shape)


#deep learning
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D





model=Sequential()
#adding more layrs
model.add(Conv2D(64,(3,3),input_shape=x_tr.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation("relu"))
          
    
model.add(Dense(64))
model.add(Activation("relu"))
          
model.add(Dense(10))
model.add(Activation("softmax"))




model.summary()






model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])
              
              
model.fit(x_tr,y_tr,epochs=5,validation_split=0.3) 



#accuracy and loss
test_loss,test_accur=model.evaluate(x_ts,y_ts)


print(test_loss)
print(test_accur)

print(pred)

print(np.argmax(pred[0]))
plt.imshow(x_ts[0])




